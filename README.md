# GPT Detector ğŸ¤–

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)

A robust AI-generated text detection system powered by fine-tuned RoBERTa transformers. This tool helps distinguish between human-written and machine-generated content with high accuracy.

## ğŸŒŸ Features

- **High Accuracy Detection**: Fine-tuned RoBERTa model for reliable AI text detection
- **Easy-to-Use API**: Simple Flask-based REST API for integration
- **Multiple Deployment Options**: 
  - Local inference
  - Flask web application
  - AWS SageMaker deployment
  - AWS Lambda serverless deployment
- **Comprehensive Training Pipeline**: Full training infrastructure with AWS SageMaker support
- **Production Ready**: Includes Docker configurations and Terraform IaC for cloud deployment

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage](#usage)
  - [Local Inference](#local-inference)
  - [Web Application](#web-application)
  - [API Usage](#api-usage)
- [Training](#training)
- [Deployment](#deployment)
- [Project Structure](#project-structure)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- (Optional) CUDA-compatible GPU for faster inference

### Basic Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/gpt-detector.git
cd gpt-detector

# Install dependencies
pip install -r requirements.txt

# Install the package in development mode
pip install -e .
```

### Docker Installation

```bash
# Build the inference container
cd deployment/docker
docker build -f Dockerfile.inference -t gpt-detector:latest .

# Run the container
docker run -p 5000:5000 gpt-detector:latest
```

## ğŸ¯ Quick Start

### Using the Python API

```python
from gpt_detector.model import ROBERTAClassifier
from gpt_detector.utils import load_checkpoint, parse_arge
from transformers import RobertaTokenizer
import torch

# Load the model
args = parse_arge()
model = ROBERTAClassifier(n_classes=2, dropout_rate=0.3)
model = load_checkpoint("path/to/model.pkl", model)

# Tokenize input
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
text = "Your text to analyze here"
encoded = tokenizer(text, padding="max_length", truncation=True, return_tensors="pt")

# Make prediction
model.eval()
with torch.no_grad():
    output = model(input_ids=encoded['input_ids'], attention_mask=encoded['attention_mask'])
    prediction = torch.argmax(output, dim=-1).item()

print("Machine generated" if prediction == 0 else "Human generated")
```

### Using the Web Interface

```bash
# Start the Flask application
python src/gpt_detector/app.py

# Open your browser and navigate to
# http://localhost:5000
```

### Using the REST API

```bash
# Start the server
python src/gpt_detector/app.py

# Make a prediction request
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "Your text to analyze here"}'
```

## ğŸ“– Usage

### Local Inference

Run inference on a single text sample:

```bash
python examples/evaluation.py --config src/gpt_detector/config/hyperparameters.json
```

### Web Application

The web application provides a user-friendly interface for text analysis:

1. Start the Flask server:
```bash
python src/gpt_detector/app.py
```

2. Open your browser to `http://localhost:5000`
3. Enter your text in the input box
4. Click "Analyze" to get the prediction

### API Usage

#### Health Check

```bash
curl http://localhost:5000/health
```

#### Predict (Form Data)

```bash
curl -X POST http://localhost:5000/ \
  -F "text=Your text to analyze here"
```

#### Predict (JSON)

```bash
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "Your text to analyze here"}'
```

Response:
```json
{
  "message": "Success",
  "input_data": "human generated"
}
```

## ğŸ‹ï¸ Training

### Local Training

1. Prepare your dataset:
```bash
python src/gpt_detector/data_process.py --config src/gpt_detector/config/hyperparameters.json
```

2. Train the model:
```bash
python src/gpt_detector/training/train.py --config src/gpt_detector/config/hyperparameters.json
```

### AWS SageMaker Training

1. Build and push the training Docker image:
```bash
./scripts/build_and_push_train.sh
```

2. Launch the training pipeline:
```bash
python src/gpt_detector/training/pipeline_train.py
```

3. Monitor training progress in the AWS SageMaker Console

### Training Configuration

Edit `src/gpt_detector/config/hyperparameters.json` to customize training:

```json
{
    "saved_model_dir": "/opt/ml/model",
    "datafolder": "data",
    "output_path": "/opt/ml/model",
    "epochs": 10,
    "train_batch_size": 16,
    "test_batch_size": 16,
    "lr": 5e-5,
    "seed": 42,
    "dropout_rate": 0.3
}
```

## ğŸš¢ Deployment

### AWS SageMaker Deployment

```bash
# Build and push inference image
./scripts/build_and_push_inference.sh

# Deploy the model
python src/gpt_detector/training/step_deploy.py
```

### AWS Lambda Deployment

```bash
# Build and push Lambda image
./scripts/build_and_push_lambda.sh

# Deploy using Terraform
cd deployment/terraform
terraform init
terraform plan
terraform apply
```

### Docker Deployment

```bash
# Build the image
docker build -f deployment/docker/Dockerfile.inference -t gpt-detector:latest .

# Run the container
docker run -d -p 5000:5000 --name gpt-detector gpt-detector:latest
```

## ğŸ“ Project Structure

```
gpt-detector/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ gpt_detector/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ model.py              # RoBERTa classifier model
â”‚       â”œâ”€â”€ utils.py              # Utility functions
â”‚       â”œâ”€â”€ data_process.py       # Data preprocessing
â”‚       â”œâ”€â”€ app.py                # Flask web application
â”‚       â”œâ”€â”€ config/
â”‚       â”‚   â””â”€â”€ hyperparameters.json
â”‚       â””â”€â”€ training/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ train.py          # Training script
â”‚           â”œâ”€â”€ inference.py      # SageMaker inference
â”‚           â”œâ”€â”€ pipeline_train.py # SageMaker pipeline
â”‚           â””â”€â”€ step_deploy.py    # Deployment script
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ evaluation.py             # Example inference script
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_dataprocess.py
â”‚   â”œâ”€â”€ test_model.py
â”‚   â”œâ”€â”€ test_train_roberta.py
â”‚   â””â”€â”€ test_utils.py
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile.inference
â”‚   â”‚   â”œâ”€â”€ Dockerfile.train
â”‚   â”‚   â””â”€â”€ Dockerfile.lambda
â”‚   â”œâ”€â”€ terraform/
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ handler.py                # Lambda handler
â”‚   â””â”€â”€ lambda-sagemaker-api.py   # Lambda SageMaker API
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_and_push_train.sh
â”‚   â”œâ”€â”€ build_and_push_inference.sh
â”‚   â”œâ”€â”€ build_and_push_lambda.sh
â”‚   â””â”€â”€ build_model.sh
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ postman/                  # API documentation
â”œâ”€â”€ data/                         # Training data
â”œâ”€â”€ saved_models/                 # Model checkpoints
â”œâ”€â”€ templates/                    # HTML templates
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ CONTRIBUTING.md
```

## ğŸ§ª Testing

Run the test suite:

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=src/gpt_detector --cov-report=html
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details on how to:

- Report bugs
- Suggest enhancements
- Submit pull requests
- Follow our code style

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built with [Hugging Face Transformers](https://huggingface.co/transformers/)
- Uses the [RoBERTa](https://arxiv.org/abs/1907.11692) model architecture
- Training data from [Yelp Review Full dataset](https://huggingface.co/datasets/yelp_review_full) (modified for binary classification)

## ğŸ“Š Model Performance

The model achieves strong performance on distinguishing between human-written and AI-generated text:

- **Accuracy**: ~XX% (update with your metrics)
- **Precision**: ~XX%
- **Recall**: ~XX%
- **F1 Score**: ~XX%

## ğŸ—ºï¸ Roadmap

- [ ] Add support for multiple languages
- [ ] Implement model explainability features (attention visualization)
- [ ] Create a browser extension for real-time detection
- [ ] Add support for detecting specific AI models (GPT-3, GPT-4, Claude, etc.)
- [ ] Improve model performance with larger datasets
- [ ] Add batch processing capabilities
- [ ] Create a public demo website

## ğŸ“ Contact & Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/gpt-detector/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/gpt-detector/discussions)

---

Made with â¤ï¸ by the GPT Detector team
